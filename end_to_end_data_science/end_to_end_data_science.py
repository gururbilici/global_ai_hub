# -*- coding: utf-8 -*-
"""End_to_End_Data_Science.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fk8JjWFhclX2PPTgSQDIRcpzg7QiuYMU
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt

# %matplotlib inline

from pandas_profiling import ProfileReport
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import math
from sklearn.linear_model import Ridge
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import KFold, cross_val_score
from sklearn.svm import SVR
import xgboost as xgb
from sklearn.preprocessing import MinMaxScaler
import warnings
warnings.filterwarnings("ignore", category=Warning)

df_insurance=pd.read_csv("insurance.csv")

df_insurance.head()

df_insurance.tail()

"""**Keşifsel Veri Analizi Yapılması**

## Keşifsel Veri Analizi Yapılması ##
"""

## Keşifsel Veri Analizi Yapılması

## Bmi(Vücut Kitle İndeksi)’nin dağılımını
df_insurance["bmi"].value_counts()

# “smoker” ile “charges” arasındaki ilişki
# yes -> 1 no -> 0
smoker=df_insurance["smoker"]
smoker_categorical=smoker.astype('category').cat.codes
charges=df_insurance["charges"]

print(f"correlation between Smoker and Charges: {charges.corr(smoker_categorical)}")

"""Smoker ve Charges feature’ların korelasyon ilişkisine baktığımızda 0.787251430498478’lik bir oranın olduğunu görüyoruz yani burada güçlü bir korelasyon ilişkisinde bahsedebiliriz."""



# “smoker” ile region arasındaki ilişki

region=df_insurance["region"]
region_categorical=region.astype('category').cat.codes
region_labels=dict(zip(region.values,region_categorical))

print(f"correlation between Smoker and Region:{smoker_categorical.corr(region_categorical)}")

"""Smoker ve Region feature’ların korelasyon ilişkisine baktığımızda -0.002180682040934647 bir oranın olduğunu görüyoruz yani burada korelasyondan bahsedemeyiz"""



# bmi ile sex arasındaki ilişki
gender=df_insurance["sex"]
gender_categorical=gender.astype('category').cat.codes
gender_labels=dict(zip(gender.values,gender_categorical))

bmi=df_insurance["bmi"]

print(f"correlation between bmi and sex:{gender_categorical.corr(bmi)}")

"""Bmi ve Sex feature’ların korelasyon ilişkisine baktığımızda 0.046371150646294566 bir oranın olduğunu görüyoruz yani burada korelasyondan bahsedemeyiz"""

# En çok “children”’a sahip “region”’ı bulunuz.
region_children=df_insurance.groupby("region")["children"].sum()

print(f"en çok çocuğa'a sahip bölge {region_children.idxmax()}")
print("")
print(f"bölgelerin sahip olduğu çocuk sayıları:{region_children}")

#“Age” ile “bmi” arasındaki ilişkiyi inceleyiniz.
age=df_insurance["age"]
print(f"correlation between bmi and age:{age.corr(bmi)}")

"""Age ve Bmi feature’ların korelasyon ilişkisine baktığımızda 0.1092718815485352 bir oranın olduğunu görüyoruz yani burada korelasyondan bahsedemeyiz"""

#children ile “bmi” arasındaki ilişkiyi inceleyiniz.
children=df_insurance["children"]
print(f"correlation between bmi and children:{children.corr(bmi)}")

"""Children ve Bmi feature’ların korelasyon ilişkisine baktığımızda 0.012758900820673817 bir oranın olduğunu görüyoruz yani burada korelasyondan bahsedemeyiz"""

#“bmi” değişkeninde outlier var mıdır? İnceleyiniz.
fig, ax = plt.subplots()
ax.boxplot(bmi)
plt.show()

"""Evet outlier vardır. Aşağıdaki boxplot'ı incelediğimizde maximum değerden daha fazla olan değerler olduğunu görüyoruz."""

#“bmi” ile “charges” arasındaki ilişkiyi inceleyiniz.

print(f"correlation between bmi and charges:{bmi.corr(charges)}")

"""Charges ve Bmi feature’ların korelasyon ilişkisine baktığımızda :0.19834096883362895 bir oranın olduğunu görüyoruz yani burada korelasyondan bahsedemeyiz

**4. Veri Ön İşleme Yapılması**
"""

df_insurance

df_insurance.duplicated().value_counts()

df_insurance.drop_duplicates(keep="first", inplace=True)

df_insurance.duplicated().value_counts()

df_insurance.isnull().sum()

df_insurance.head(20)

label_encoder = preprocessing.LabelEncoder()  
df_insurance['sex']= label_encoder.fit_transform(df_insurance['sex'])
df_insurance['smoker']= label_encoder.fit_transform(df_insurance['smoker'])
df_insurance['region']= label_encoder.fit_transform(df_insurance['region'])

features=df_insurance.iloc[:,:-1].values ## Features
target=df_insurance.iloc[:,-1].values## Target

features.shape

target.shape

target=target.reshape(-1,1)

target.shape

X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2,shuffle=True)

X_train.shape

X_test.shape

"""## Modelling ##

**Linear Regression**
"""

reg = LinearRegression()
reg.fit(X_train,y_train)
predictions=reg.predict(X_test)

MSE = mean_squared_error(y_test, predictions)
RMSE = math.sqrt(MSE)
print(f"Mean Square Error:{MSE}")
print(f"Root Mean Square Error:{RMSE}")

"""**Linear Regression- Cross Validation**"""

reg_cross_validation = LinearRegression()
cv = KFold(n_splits=5, random_state=42, shuffle=True)

#use k-fold CV to evaluate model
scores = cross_val_score(reg_cross_validation, features, target, scoring='neg_root_mean_squared_error',
                         cv=cv)

#view mean absolute error
from statistics import mean
print(f"RMSE of Linear Regression with Cross Validation:{mean(abs(scores))}")

plt.figure(figsize=(10, 10))
plt.scatter(df_insurance.index[-268:],y=y_test)
plt.plot(df_insurance.index[-268:],predictions,color="green")
plt.title( "Linear Regression")
plt.xlabel("Data index")
plt.ylabel("Charges")
plt.legend(["Predictions","Real Values"])
plt.show()

"""**Ridge Regression**"""

ridge_reg = Ridge(alpha=0.05, normalize=True)
ridge_reg.fit(X_train, y_train)
predictions_ridge=ridge_reg.predict(X_test)

MSE = mean_squared_error(y_test, predictions_ridge)
RMSE = math.sqrt(MSE)
print(f"Mean Square Error:{MSE}")
print(f"Root Mean Square Error:{RMSE}")

plt.figure(figsize=(10, 10))
plt.scatter(df_insurance.index[-268:],y=y_test)
plt.plot(df_insurance.index[-268:],predictions_ridge,color="green")
plt.title( "Ridge Regression")
plt.xlabel("Data index")
plt.ylabel("Charges")
plt.legend(["Predictions","Real Values"])
plt.show()

plt.show()

"""**Ridge Regression With Cross Validation**"""

ridge_reg_cross_validation = Ridge(alpha=0.05, normalize=True)

cv_ridge= KFold(n_splits=5, random_state=42, shuffle=True)

#use k-fold CV to evaluate model
scores_ridge = cross_val_score(ridge_reg_cross_validation, features, target, scoring='neg_root_mean_squared_error',
                         cv=cv)
print(f"RMSE of Ridge Regression with Cross Validation:{mean(abs(scores_ridge))}")



"""**Support Vector Regression**"""

## Gaussian
regressor_gaussian = SVR(kernel='rbf')
regressor_gaussian.fit(X_train,y_train)
predictions_gaussian=regressor_gaussian.predict(X_test)

## Polynomial
regressor_polynomial = SVR(kernel='poly')
regressor_polynomial.fit(X_train,y_train)
predictions_polynomial=regressor_polynomial.predict(X_test)

## Linear
regressor_linear = SVR(kernel='linear')
regressor_linear.fit(X_train,y_train)
predictions_linear=regressor_linear.predict(X_test)

MSE = mean_squared_error(y_test, predictions_gaussian)
RMSE = math.sqrt(MSE)
print(f"Mean Square Error of Gaussian:{MSE}")
print(f"Root Mean Square Error of Gaussian :{RMSE}")

MSE = mean_squared_error(y_test, predictions_polynomial)
RMSE = math.sqrt(MSE)
print(f"Mean Square Error of Polynomial:{MSE}")
print(f"Root Mean Square Error of Polynomial :{RMSE}")

MSE = mean_squared_error(y_test, predictions_linear)
RMSE = math.sqrt(MSE)
print(f"Mean Square Error of Linear:{MSE}")
print(f"Root Mean Square Error of Linear :{RMSE}")

"""**Support Vector Regression with Cross Validation**"""

gaussian_cross_validation= SVR(kernel='rbf')

cv_gaussian = KFold(n_splits=5, random_state=42, shuffle=True)

#use k-fold CV to evaluate model
scores_gaussian = cross_val_score(regressor_gaussian, features, target, scoring='neg_root_mean_squared_error',
                         cv=cv_gaussian)
print(f"RMSE of Gaussian with Cross Validation:{mean(abs(scores_gaussian))}")

linear_cross_validation = SVR(kernel='linear')

cv_linear = KFold(n_splits=5, random_state=42, shuffle=True)

#use k-fold CV to evaluate model
scores_linear = cross_val_score(linear_cross_validation, features, target, scoring='neg_root_mean_squared_error',
                         cv=cv_linear)
print(f"RMSE of Linear with Cross Validation:{mean(abs(scores_linear))}")

polynomial_cross_validation = SVR(kernel='poly')

cv_polynomial = KFold(n_splits=5, random_state=42, shuffle=True)

#use k-fold CV to evaluate model
scores_polynomial = cross_val_score(polynomial_cross_validation, features, target, scoring='neg_root_mean_squared_error',
                         cv=cv_polynomial)
print(f"RMSE of Polynomial with Cross Validation:{mean(abs(scores_polynomial))}")

"""**XGBOOST**"""

model_xgb = xgb.XGBRegressor()
model_xgb.fit(X_train, y_train)

predictions_xgb=model_xgb.predict(X_test)

MSE = mean_squared_error(y_test, predictions_xgb)
RMSE = math.sqrt(MSE)
print(f"Mean Square Error of XGBOOST:{MSE}")
print(f"Root Mean Square Error of XGBOOST :{RMSE}")

"""**XGBOOST-Cross Validation**"""

model_xgb_cross_validation = xgb.XGBRegressor()

cv_xgb = KFold(n_splits=5, random_state=42, shuffle=True)

#use k-fold CV to evaluate model
scores_xgb = cross_val_score(model_xgb_cross_validation, features, target, scoring='neg_root_mean_squared_error',
                         cv=cv_xgb)
print(f"RMSE of XGBOOST with Cross Validation:{mean(abs(scores_xgb))}")

"""## Grid Search ##

**Linear Regression**
"""

parameters = {"fit_intercept": [True,False],
              "normalize": [True, False],
              "n_jobs": [5,10,15,20,-1], 
              "positive":[True,False]
             }
from sklearn.model_selection import GridSearchCV



reg = LinearRegression()
cv = KFold(n_splits=5, random_state=42, shuffle=True)
grid = GridSearchCV(estimator=reg, param_grid=parameters, scoring='neg_root_mean_squared_error', verbose=1,cv=cv)
grid_result = grid.fit(X_train, y_train)
print('Best Score: ', abs(grid_result.best_score_))
print('Best Params: ', grid_result.best_params_)

"""**Ridge Regression**"""

parameters_ridge = {"alpha": [1, 10, 100, 290, 500],
              "fit_intercept": [True, False],
              "solver": ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'], 
              "max_iter":[5,10,15,20,25,30,35,40,45,50],
              "normalize":[True,False]
             }

ridge = Ridge()
grid = GridSearchCV(estimator=ridge, param_grid=parameters_ridge, scoring='neg_root_mean_squared_error', verbose=1,cv=cv_ridge)
grid_result = grid.fit(X_train, y_train)
print('Best Score: ', abs(grid_result.best_score_))
print('Best Params: ', grid_result.best_params_)

"""**Support Vector Regression**"""

parameters_SVR = [
    {"kernel": ["rbf"], "gamma": [1e-3, 1e-4], "C": [1, 10, 100, 1000]},
    {"kernel": ["linear"], "C": [1, 10, 100, 1000]},
    {"kernel": ["poly"], "C": [1, 10, 100, 1000]}
]

svr_model=SVR()
cv_svr = KFold(n_splits=5, random_state=42, shuffle=True)
grid = GridSearchCV(estimator=svr_model, param_grid=parameters_SVR, scoring='neg_root_mean_squared_error', verbose=1,cv=cv_svr)
grid_result = grid.fit(X_train, y_train)
print('Best Score: ', abs(grid_result.best_score_))
print('Best Params: ', grid_result.best_params_)

"""**XGBOOST**"""

parameters_xgb = {'max_depth': [2,4,6],
                  'n_estimators': [50,100,200]}

model_xgb = xgb.XGBRegressor()
cv_xgb = KFold(n_splits=5, random_state=42, shuffle=True)
grid = GridSearchCV(estimator=model_xgb, param_grid=parameters_xgb, scoring='neg_root_mean_squared_error', verbose=1,cv=cv_xgb)
grid_result = grid.fit(X_train, y_train)
print('Best Score: ', abs(grid_result.best_score_))
print('Best Params: ', grid_result.best_params_)

warnings.filterwarnings(action= 'ignore')

